version: '3.7'
############################
##  DEFAULT
############################
x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@db/airflow
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@db/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@db/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/config:/opt/airflow/config
    - ./airflow/plugins:/opt/airflow/plugins
    - ./util:/util
  user: "${AIRFLOW_UID:-50000}:0"
  
services:
############################
##  DATA LAKE
############################
  minio:
    image: minio/minio:latest
    platform: linux/amd64
    container_name: minio
    entrypoint: sh
    command:   '-c ''mkdir -p /minio_data/raw && mkdir -p /minio_data/trusted && minio server /minio_data --console-address ":9001"'''
    ports:
      - "9050:9000"
      - "9051:9001"
    hostname: minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_ACCESS_KEY: datalake
      MINIO_SECRET_KEY: datalake
    volumes:
      - ./minio/data1:/data


  namenode:
    image: fjardim/mds-namenode
    platform: linux/amd64
    container_name: namenode
    hostname: namenode
    volumes:
      - ./hadoop/hdfs/namenode:/hadoop/dfs/name
      - ./hadoop/util:/util
    env_file:
      - ./hadoop/hadoop.env
    ports:
      - "9870:9870"
    deploy:
      resources:
        limits:
          memory: 500m
  
  datanode:
    image: fjardim/mds-datanode
    platform: linux/amd64
    container_name: datanode
    hostname: datanode
    volumes:
      - ./hadoop/hdfs/datanode:/hadoop/dfs/data
      - ./hadoop/util:/util
    env_file:
      - ./hadoop/hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    deploy:
      resources:
        limits:
          memory: 500m
############################
## STREAMING
############################
  zookeeper:
    image: fjardim/mds-kafka-zookeeper
    platform: linux/amd64
    hostname: zookeeper
    container_name: zookeeper
    ports:
       - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    

  kafka-broker:
    image: fjardim/mds-kafka-broker
    platform: linux/amd64
    container_name: kafka-broker
    hostname: kafka-broker
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper

    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka-broker:9092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka-broker
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
      # AUTO_REGISTER_SCHEMAS : 'false'
    

  kafka-schema-registry:
    image: fjardim/mds-kafka-schema-registry
    platform: linux/amd64
    hostname: kafka-schema-registry
    container_name: kafka-schema-registry
    depends_on:
      - kafka-broker
      
    ports:
      - "8071:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
  

  kafka-connect:
    image: fjardim/mds-kafka-connect
    platform: linux/amd64
    hostname: kafka-connect
    container_name: kafka-connect
    depends_on:
      - kafka-broker
      - kafka-schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081
      # CLASSPATH required due to CC-2422
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.1.0.jar
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      #CONNECT_REST_ADVERTISED_HOST_NAME: localhost
          
  kafka-control-center:
    image: fjardim/mds-kafka-control-center
    platform: linux/amd64
    hostname: kafka-control-center
    container_name: kafka-control-center
    depends_on:
      - kafka-broker
      - kafka-schema-registry
      - kafka-connect
      - kafka-ksqldb-server
      - kafka-ksqldb-cli
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      CONTROL_CENTER_CONNECT_CLUSTER: 'kafka-connect:8083'
      CONTROL_CENTER_CONNECT_CONNECT-DEFAULT_CLUSTER: 'kafka-connect:8083'
      CONTROL_CENTER_KSQL_KSQLDB1_URL: "http://kafka-ksqldb-server:8088"
      CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: "http://kafka-ksqldb-server:8088"
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://kafka-schema-registry:8081"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONTROL_CENTER_CONNECT_HEALTHCHECK_ENDPOINT: '/connectors'
      PORT: 9021

  kafka-ksqldb-server:
    image: fjardim/mds-kafka-ksqldb-server
    platform: linux/amd64
    hostname: kafka-ksqldb-server
    container_name: kafka-ksqldb-server
    depends_on:
      - kafka-broker
      - kafka-connect
    ports:
      - "8068:8088"
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_BOOTSTRAP_SERVERS: "kafka-broker:9092"
      KSQL_HOST_NAME: ksqldb-server
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://kafka-schema-registry:8081"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_CONNECT_URL: "http://kafka-connect:8083"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'

  kafka-ksqldb-cli:
    image: fjardim/mds-kafka-sqldb-cli
    platform: linux/amd64
    container_name: kafka-ksqldb-cli
    hostname: kafka-ksqldb-cli
    depends_on:
      - kafka-broker
      - kafka-connect
      - kafka-ksqldb-server
    entrypoint: /bin/sh
    tty: true
  
  kafka-rest-proxy:
    image: fjardim/mds-kafka-rest
    platform: linux/amd64
    hostname: kafka-rest-proxy
    depends_on:
      - kafka-broker
      - kafka-schema-registry
    ports:
      - 8082:8082
    #hostname: rest-proxy
    container_name: kafka-rest-proxy
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'

############################
## INGESTAO
############################

  nifi:
      image: apache/nifi:latest
      platform: linux/amd64
      container_name: nifi
      hostname: nifi
      volumes:
        - ./nifi/util:/util
        #- ./nifi/database_repository:/opt/nifi/nifi-current/database_repository
        #- ./nifi/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
        #- ./nifi/content_repository:/opt/nifi/nifi-current/content_repository
        #- ./nifi/provenance_repository:/opt/nifi/nifi-current/provenance_repository
        #- ./nifi/state:/opt/nifi/nifi-current/state
        #- ./nifi/conf:/opt/nifi/nifi-current/conf
        #- ./util:/util
      environment:
        NIFI_WEB_HTTP_PORT: "9090"
        NIFI_WEB_HTTPS_HOST: "nifi"
        TZ: "America/Sao_Paulo"
      command: >
        sh -c "ln -snf /usr/share/zoneinfo/$$TZ /etc/localtime && echo $$TZ > /etc/timezone"
      
      ports:
        - 49090:9090
      deploy:
        resources:
          limits:
            memory: 2g
            #cpus: '0.2'

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    platform: linux/amd64
    hostname: airflow-webserver
    command: webserver
    ports:
      - "58080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - redis
      - db
      - airflow-scheduler
      - airflow-worker

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    platform: linux/amd64
    hostname: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - redis
      - db

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    platform: linux/amd64
    hostname: airflow-worker
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    depends_on:
      - redis
      - db

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    platform: linux/amd64
    hostname: airflow-triggerer
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - redis
      - db

  airflow-cli:
    <<: *airflow-common
    container_name: airflow-cli
    platform: linux/amd64
    hostname: airflow-cli
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  airflow-flower:
    <<: *airflow-common
    container_name: airflow-flower
    platform: linux/amd64
    hostname: airflow-flower
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s


############################
## DATABASE
############################

  redis:
    image: redis:latest
    platform: linux/amd64
    container_name: redis
    hostname: redis
    expose:
      - 6379
    ports:
        - 6379:6379
    volumes:
      - ./util:/util    

  presto:
    platform: linux/amd64
    image: prestodb/presto
    hostname: presto
    container_name: presto
    volumes: 
      - ./presto/etc/catalog:/opt/presto-server/etc/catalog/
      - ./presto/etc/catalog:/opt/presto/etc/catalog/
      - ./presto/etc/catalog:/etc/catalog/ 
      - ./presto/etc/hadoop:/hadoop 
      - ./util:/util
    ports:
      - 18080:8080
    depends_on:
      - hive

    deploy:
      resources:
        limits:
          memory: 2g
          #cpus: '0.2'

  db:
    image: postgres
    platform: linux/amd64
    container_name: db
    hostname: db
    environment:
      POSTGRES_PASSWORD: admin
      POSTGRES_USER: admin
      POSTGRES_DB: admin
    command: postgres -c shared_preload_libraries=pg_stat_statements -c pg_stat_statements.track=all -c max_connections=200 -c wal_level=logical
    ports:
      - 15432:5432
    volumes:
      - ./postgres/volume:/var/lib/postgresql/data
      - ./util:/util

  adminer:
    image: adminer
    platform: linux/amd64
    container_name: adminer
    hostname: adminer
    ports:
      - 28080:8080
      
  mysql:
    image: mysql
    platform: linux/amd64
    container_name: mysql
    # NOTE: use of "mysql_native_password" is not recommended: https://dev.mysql.com/doc/refman/8.0/en/upgrading-from-previous-series.html#upgrade-caching-sha2-password
    # (this is just an example, not intended to be a production configuration)
    command: --default-authentication-plugin=mysql_native_password
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: root
    ports:
      - 13306:3306
    volumes:
      - ./mysql/db:/var/lib/mysql


  mongo:
    image: mongo
    platform: linux/amd64
    container_name: mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin
    ports:
      - "27017:27017"
    volumes:
      - ./mongodb/data:/data/db
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 1g
          #cpus: '0.2'

  mongo-express:
    image: mongo-express
    platform: linux/amd64
    container_name: mongo-express
    ports:
      - 28081:8081
    depends_on:
      - mongo
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: admin
      ME_CONFIG_MONGODB_ADMINPASSWORD: admin
      ME_CONFIG_MONGODB_URL: mongodb://root:root@mongo:27017/
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin

  hive:
    hostname: hive
    #image: apache/hive:4.0.0-beta-1
    image: fjardim/mds-hive
    platform: linux/amd64
    container_name: hive
    environment:
      AWS_ACCESS_KEY_ID: datalake
      AWS_SECRET_ACCESS_KEY: datalake
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: hiveserver2
      SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083" 
      IS_RESUME: "true"
      #HIVE_VERSION: "3.1.3"
    ports:
       - "10000:10000"
       - "10002:10002"
    depends_on:
      - metastore
    user: root
    volumes:
       - ./hive/conf:/hive_custom_conf
       - ./util:/util
          
  metastore:
    hostname: metastore
    platform: linux/amd64
    image: fjardim/mds-hive-metastore
    #image: apache/hive:4.0.0-beta-1
    container_name: metastore
    environment:
      AWS_ACCESS_KEY_ID: datalake
      AWS_SECRET_ACCESS_KEY: datalake
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: metastore
      #SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083" 
      IS_RESUME: "true"
      DB_DRIVER: postgres 
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://db:5432/metastore -Djavax.jdo.option.ConnectionUserName=admin -Djavax.jdo.option.ConnectionPassword=admin" 
    ports:
       - "9083:9083"
    depends_on:
        - db
          
    user: root
    volumes:
       - ./hive/meta:/opt/hive/data/warehouse 
       - ./hive/conf:/hive_custom_conf
       - ./util:/util

  elasticsearch:
    image: elasticsearch:7.10.1
    platform: linux/amd64
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      discovery.type: "single-node"
      ES_JAVA_OPTS: "-Xms2g -Xmx2g"
      #ELASTIC_PASSWORD: "12345"
      xpack.security.enabled: "false"
    volumes:
      - ./elasticsearch/esdata:/usr/share/elasticsearch/data
      - ./util:/util


  cassandra:
    image: cassandra:latest
    platform: linux/amd64
    container_name: cassandra
    hostname: cassandra
    environment:
      CQLSH_HOST: cassandra 
      CQLSH_PORT: 9042 
      CQLVERSION: 3.4.6
      CASSANDRA_CLUSTER_NAME: datalab
      
    ports:
      - "9042:9042"
      - "17000:7000"
      - "10001:10000"
    volumes:
      - ./util:/util
      - ./cassandra/data/:/var/lib/cassandra

  cassandra-web:
    image: ipushc/cassandra-web
    platform: linux/amd64
    container_name: cassandra-web
    depends_on:
      - cassandra
    ports:
      - 13000:80
    environment:
      CASSANDRA_HOST: cassandra
      CASSANDRA_PORT: 9042
      CASSANDRA_USER: cassandra
      CASSANDRA_PASSWORD: cassandra 
      HOST_PORT: ":80"  

  clickhouse:
    image: clickhouse/clickhouse-server
    platform: linux/amd64
    container_name: clickhouse
    hostname: clickhouse
    ports:
      - "18123:8123"
      - "29000:9000"
    environment:
      CLICKHOUSE_DB: "datalab"
      CLICKHOUSE_USER: "admin"
      CLICKHOUSE_PASSWORD: "admin"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
    volumes:
      #- ./clickhouse/data:/var/lib/clickhouse/ 
      #- ./clickhouse/config/config.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./util:/util

############################
## VISUALIZACAO
############################

  metabase:
      image: metabase/metabase:latest
      platform: linux/amd64
      container_name: metabase
      hostname: metabase
      ports:
        - 3000:3000
      depends_on:
        - db
      environment:
        MB_DB_TYPE: postgres
        MB_DB_DBNAME: metabase
        MB_DB_PORT: 5432
        MB_DB_PASS: admin
        MB_DB_USER: admin
        MB_DB_HOST: db
        MB_PASSWORD_COMPLEXITY: "weak"
        MB_PASSWORD_LENGTH: 4
        #user user@datalab.com.br pass datalab
      volumes:
        - ./util:/util

      
  kibana:
    image: kibana:7.10.1
    platform: linux/amd64
    container_name: kibana
    hostname: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      #ELASTICSEARCH_USERNAME: "elastic"
      #ELASTICSEARCH_PASSWORD: "12345"
      #ELASTICSEARCH_SERVICEACCOUNTTOKEN: "GgXkNtUyT2CjF5k_488JcA"
    depends_on:
      - elasticsearch
 
##################################################
### ANALISE E PROCESSAMENTO
##################################################
  spark-master:
    image: fjardim/mds-spark
    platform: linux/amd64
    hostname: spark-master
    container_name: spark-master
    command: 
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-master.sh
        start-notebook.sh --NotebookApp.token=''
    ports:
      - 8889:8888
      - 4040:4040
      - 4041:4041
      - 4042:4042
      - 4043:4043
      - 38080:8080
      - 7077:7077
    volumes:
      - ./spark/work:/home/user 
      - ./spark/env:/env 
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 2g

  spark-worker:
    image: fjardim/mds-spark
    platform: linux/amd64
    hostname: spark-worker
    container_name: spark-worker
    command: 
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-worker.sh spark-master:7077
        start-notebook.sh --NotebookApp.token='' 
    #command: /usr/local/spark/sbin/start-worker.sh jupyter-spark:7077
    #environment:
    #  PYSPARK_SUBMIT_ARGS: "--packages io.delta:delta-core_2.12:2.2.0,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.392"
    env_file:
      - ./spark/env/jupyter.env
    ports:
      - 5040:4040
      - 5041:4041
      - 5042:4042
      - 5043:4043
      - 38081:8081
      - 36533:36533
    volumes:
      - ./util:/util
      - ./spark/work:/home/user 
    environment:
      SPARK_MASTER: spark-master
    depends_on:
        - spark-master
    deploy:
      resources:
        limits:
          memory: 1g

##################################################
### GOVERNANCA
##################################################
  mysql-setup:
    depends_on:
      mysql:
        condition: service_healthy
    environment:
    - MYSQL_HOST=mysql
    - MYSQL_PORT=3306
    - MYSQL_USERNAME=root
    - MYSQL_PASSWORD=root
    - DATAHUB_DB_NAME=datahub
    hostname: mysql-setup
    image: ${DATAHUB_MYSQL_SETUP_IMAGE:-acryldata/datahub-mysql-setup}:${DATAHUB_VERSION:-head}
    container_name: datahub-mysql-setup
    
  kafka-setup:
    depends_on:
      kafka-broker:
        condition: service_healthy
      kafka-schema-registry:
        condition: service_healthy
    environment:
    - DATAHUB_PRECREATE_TOPICS=${DATAHUB_PRECREATE_TOPICS:-false}
    - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
    - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    - USE_CONFLUENT_SCHEMA_REGISTRY=FALSE
    hostname: kafka-setup
    image: ${DATAHUB_KAFKA_SETUP_IMAGE:-linkedin/datahub-kafka-setup}:${DATAHUB_VERSION:-head}
    container_name: datahub-kafka-setup

  elasticsearch-setup:
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
    - ELASTICSEARCH_USE_SSL=${ELASTICSEARCH_USE_SSL:-false}
    - USE_AWS_ELASTICSEARCH=${USE_AWS_ELASTICSEARCH:-false}
    - ELASTICSEARCH_HOST=elasticsearch
    - ELASTICSEARCH_PORT=9200
    - ELASTICSEARCH_PROTOCOL=http
    hostname: elasticsearch-setup
    image: ${DATAHUB_ELASTIC_SETUP_IMAGE:-linkedin/datahub-elasticsearch-setup}:${DATAHUB_VERSION:-head}
    container_name: datahub-elasticsearch-setup

  datahub-actions:
    depends_on:
      datahub-gms:
        condition: service_healthy
    environment:
      - ACTIONS_CONFIG=${ACTIONS_CONFIG:-}
      - ACTIONS_EXTRA_PACKAGES=${ACTIONS_EXTRA_PACKAGES:-}
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_GMS_PROTOCOL=http
      - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
      - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
      - KAFKA_PROPERTIES_SECURITY_PROTOCOL=PLAINTEXT
      - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
      - METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME=MetadataChangeLog_Versioned_v1
      - SCHEMA_REGISTRY_URL=http://kafka-schema-registry:8081
    hostname: actions
    image: ${DATAHUB_ACTIONS_IMAGE:-acryldata/datahub-actions}:${ACTIONS_VERSION:-head}
    container_name: datahub-actions

  datahub-frontend-react:
    depends_on:
      datahub-gms:
        condition: service_healthy
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_SECRET=YouKnowNothing
      - DATAHUB_APP_VERSION=1.0
      - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
      - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml -Dlogback.debug=false -Dpidfile.path=/dev/null
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
      - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
      - ELASTIC_CLIENT_HOST=elasticsearch
      - ELASTIC_CLIENT_PORT=9200
    hostname: datahub-frontend-react
    image: ${DATAHUB_FRONTEND_IMAGE:-linkedin/datahub-frontend-react}:${DATAHUB_VERSION:-head}
    ports:
      - 9002:9002
    volumes:
      - ./datahub/plugins:/etc/datahub/plugins
    container_name: datahub-frontend

  datahub-gms:
    depends_on:
      datahub-upgrade:
        condition: service_completed_successfully
    environment:
      - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-quickstart}
      - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_PASSWORD=root
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_USERNAME=root
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_PORT=9200
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - ENTITY_SERVICE_ENABLE_RETENTION=true
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - GRAPH_SERVICE_DIFF_MODE_ENABLED=true
      - GRAPH_SERVICE_IMPL=elasticsearch
      - JAVA_OPTS=-Xms1g -Xmx1g
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
      - KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR=${KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR:-true}
      - KAFKA_SCHEMAREGISTRY_URL=http://kafka-schema-registry:8081
      - MAE_CONSUMER_ENABLED=true
      - MCE_CONSUMER_ENABLED=true
      - PE_CONSUMER_ENABLED=true
      - UI_INGESTION_ENABLED=true
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 90s
      test: curl -sS --fail http://datahub-gms:${DATAHUB_GMS_PORT:-8080}/health
      timeout: 5s
    hostname: datahub-gms
    image: ${DATAHUB_GMS_IMAGE:-linkedin/datahub-gms}:${DATAHUB_VERSION:-head}
    ports:
    - 8080:8080
    volumes:
    - ./datahub/plugins:/etc/datahub/plugins
    container_name: datahub-gms

  datahub-upgrade:
    command:
    - -u
    - SystemUpdate
    depends_on:
      elasticsearch-setup:
        condition: service_completed_successfully
      kafka-setup:
        condition: service_completed_successfully
      mysql-setup:
        condition: service_completed_successfully
    environment:
    - EBEAN_DATASOURCE_USERNAME=root
    - EBEAN_DATASOURCE_PASSWORD=root
    - EBEAN_DATASOURCE_HOST=mysql:3306
    - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
    - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
    - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
    - KAFKA_SCHEMAREGISTRY_URL=http://kafka-schema-registry:8081
    - ELASTICSEARCH_HOST=elasticsearch
    - ELASTICSEARCH_PORT=9200
    - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
    - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
    - ELASTICSEARCH_BUILD_INDICES_CLONE_INDICES=false
    - GRAPH_SERVICE_IMPL=elasticsearch
    - DATAHUB_GMS_HOST=datahub-gms
    - DATAHUB_GMS_PORT=8080
    - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
    - BACKFILL_BROWSE_PATHS_V2=true
    - REPROCESS_DEFAULT_BROWSE_PATHS_V2=false
    hostname: datahub-upgrade
    image: ${DATAHUB_UPGRADE_IMAGE:-acryldata/datahub-upgrade}:${DATAHUB_VERSION:-head}
    container_name: datahub-upgrade

##################################################
### EDA
#####
  grafana:
    image: "grafana/grafana"
    platform: linux/amd64
    ports:
     - "23000:3000"
    environment:
      GF_PATHS_DATA : /var/lib/grafana
      GF_SECURITY_ADMIN_PASSWORD : kafka
    volumes:
     - ./grafana/provisioning:/etc/grafana/provisioning
     - ./grafana/dashboards:/var/lib/grafana/dashboards
    container_name: grafana
    depends_on:
     - prometheus

  prometheus:
    image: "prom/prometheus"
    platform: linux/amd64
    ports:
     - "9090:9090"
    volumes:
     - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command: "--config.file=/etc/prometheus/prometheus.yml"
    container_name: prometheus

  jmx-kafka-broker:
    image: "sscaling/jmx-prometheus-exporter"
    platform: linux/amd64
    ports:
     - "5556:5556"
    environment:
     CONFIG_YML : "/etc/jmx_exporter/config.yml"
     #JVM_OPTS: ${PROMETHEUS_JMX_AGENT_JVM_OPTS}
    volumes:
     - ./jmx_exporter/config_kafka101.yml:/etc/jmx_exporter/config.yml
    container_name: jmx-kafka-broker
    depends_on:
     - kafka-broker

  zoonavigator:
    image: elkozmon/zoonavigator
    platform: linux/amd64
    container_name: zoonavigator
    ports:
      - "8000:8000"
    environment:
      HTTP_PORT: 8000
      AUTO_CONNECT_CONNECTION_STRING: zookeeper:2181
    depends_on:
      - zookeeper

  akhq:
    image: tchiotludo/akhq
    platform: linux/amd64
    container_name: akhq
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka-broker:9092"       

    ports:
      - "58080:8080"
    depends_on:
      - kafka-broker    
      - connect

  kafka-net:
      image: fernandos/kafka-net
      platform: linux/amd64
      container_name: kafkanet    
      environment:
          ASPNETCORE_ENVIRONMENT: Development
      depends_on:
          - kafka-broker       
      ports:
        - "5000:80"

  keycloak:
    image: quay.io/keycloak/keycloak:20.0.2
    platform: linux/amd64
    container_name: microcks-sso
    ports:
      - "18080:8080"
    environment:
      KEYCLOAK_ADMIN: "admin"
      KEYCLOAK_ADMIN_PASSWORD: "admin"
      KC_HOSTNAME_ADMIN_URL: "http://localhost:18080"
      KC_HOSTNAME_URL: "http://localhost:18080"
    volumes:
      - "./keycloak/microcks-realm-sample.json:/opt/keycloak/data/import/microcks-realm.json"
    command:
      - start-dev --import-realm

  postman:
    image: quay.io/microcks/microcks-postman-runtime:latest
    platform: linux/amd64
    container_name: microcks-postman-runtime

  app:
    depends_on:
      - mongo
      - keycloak
      - postman
      - kafka-broker
    image: quay.io/microcks/microcks:1.7.1
    platform: linux/amd64
    container_name: microcks
    volumes:
      - "./app/config:/deployments/config"
    ports:
      - "28080:8080"
      - "29090:9090"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
      - SPRING_DATA_MONGODB_URI=mongodb://mongo:27017
      - SPRING_DATA_MONGODB_DATABASE=microcks
      - POSTMAN_RUNNER_URL=http://postman:3000
      - TEST_CALLBACK_URL=http://microcks:8080
      - SERVICES_UPDATE_INTERVAL=0 0 0/2 * * *
      - KEYCLOAK_URL=http://keycloak:8080
      - KEYCLOAK_PUBLIC_URL=http://localhost:18080
      - ASYNC_MINION_URL=http://microcks-async-minion:8081
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092

  async-minion:
    depends_on:
      - app
      - kafka-broker
    ports:
      - "38081:8081"
    image: quay.io/microcks/microcks-async-minion:1.7.1
    platform: linux/amd64
    container_name: microcks-async-minion
    restart: on-failure
    volumes:
      - "./async-minion/config:/deployments/config"
    environment:
      - QUARKUS_PROFILE=docker-compose

  connect:
        image: fernandos/kafka-connet-debezium-lab  
        platform: linux/amd64
        container_name: kafkaConect
        ports:
        - 8083:8083
        depends_on:      
         - kafka-broker      
        environment:
        - KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:/opt/kafka/config/connect-log4j.properties
        - KAFKA_CONNECT_BOOTSTRAP_SERVERS=kafka-broker:9092
        - |
            KAFKA_CONNECT_CONFIGURATION=
            key.converter=org.apache.kafka.connect.json.JsonConverter
            value.converter=org.apache.kafka.connect.json.JsonConverter
            key.converter.schemas.enable=false
            value.converter.schemas.enable=false
            group.id=connect
            offset.storage.topic=connect-offsets
            offset.storage.replication.factor=1
            config.storage.topic=connect-configs
            config.storage.replication.factor=1
            status.storage.topic=connect-status
            status.storage.replication.factor=1  
            CONNECT_REST_ADVERTISED_HOST_NAME: 'connect'       
        command: /opt/kafka/kafka_connect_run.sh

  pgadmin:
    container_name: pgadmin_container
    image: dpage/pgadmin4
    platform: linux/amd64
    environment:
      PGADMIN_DEFAULT_EMAIL: lab-pgadmin4@pgadmin.org
      PGADMIN_DEFAULT_PASSWORD: admin    
    ports:
      - "5433:80"
    depends_on:
      - db  

#############################
## GENERAL

  

configs:
  flags:
    file: ./airbyte/flags.yml

networks:
  datalab:
    driver: bridge
